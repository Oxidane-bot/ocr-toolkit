"""
OCRæ¨¡å‹è´¨é‡å¯¹æ¯”æµ‹è¯•è„šæœ¬

å¯¹æ¯”5ä¸ªä¸åŒOCRæ¨¡å‹ç»„åˆçš„å¤„ç†è´¨é‡å’Œæ€§èƒ½ï¼š
1. è¶…å¿«é€Ÿåº¦: fast_tiny + crnn_mobilenet_v3_small (å½“å‰baseline)
2. è½»é‡å¹³è¡¡: fast_small + crnn_mobilenet_v3_large
3. ä¸­ç­‰æ€§èƒ½: linknet_resnet18 + crnn_vgg16_bn
4. é«˜è´¨é‡: db_resnet50 + vitstr_small
5. æœ€é«˜è´¨é‡: linknet_resnet50 + parseq

æ¯ä¸ªæ¨¡å‹è¿è¡Œä¸¤æ¬¡ä»¥æ¶ˆé™¤é¦–æ¬¡ä¸‹è½½æ¨¡å‹çš„æ—¶é—´å½±å“ï¼Œä½¿ç”¨ç°æœ‰çš„è´¨é‡è¯„ä¼°ç³»ç»Ÿ
"""

import argparse
import gc
import json
import logging
import os
import sys
import time
import traceback
from dataclasses import dataclass
from pathlib import Path
from typing import Any

import psutil

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), "..", ".."))

from doctr.io import DocumentFile

from ocr_toolkit import common
from ocr_toolkit.quality_evaluator import QualityEvaluator
from ocr_toolkit.utils import setup_logging_with_file


@dataclass
class ModelConfig:
    """æ¨¡å‹é…ç½®ç±»"""

    name: str
    det_arch: str
    reco_arch: str
    description: str
    expected_speed: str  # "å¾ˆå¿«", "å¿«", "ä¸­ç­‰", "æ…¢", "å¾ˆæ…¢"


@dataclass
class TestResult:
    """å•ä¸ªæµ‹è¯•ç»“æœç±»"""

    model_name: str
    file_path: str
    file_name: str
    file_size: int
    file_extension: str
    run_number: int  # 1 æˆ– 2
    success: bool
    processing_time: float
    quality_score: float
    text_length: int
    pages_processed: int
    memory_usage_mb: float
    error_message: str = ""
    quality_details: dict = None


class ModelComparisonTester:
    """OCRæ¨¡å‹å¯¹æ¯”æµ‹è¯•å™¨"""

    def __init__(self, output_dir: str = "comparison_results"):
        self.output_dir = output_dir
        self.evaluator = QualityEvaluator()
        self.results: list[TestResult] = []

        # æ¨¡å‹é…ç½®
        self.model_configs = [
            ModelConfig(
                name="è¶…å¿«é€Ÿåº¦",
                det_arch="fast_tiny",
                reco_arch="crnn_mobilenet_v3_small",
                description="å½“å‰baselineï¼Œæœ€å¿«é€Ÿåº¦ï¼Œé€‚åˆç§»åŠ¨ç«¯",
                expected_speed="å¾ˆå¿«",
            ),
            ModelConfig(
                name="è½»é‡å¹³è¡¡",
                det_arch="fast_small",
                reco_arch="crnn_mobilenet_v3_large",
                description="è½»é‡çº§å¹³è¡¡æ¨¡å‹ï¼Œé€Ÿåº¦ä¸è´¨é‡æŠ˜ä¸­",
                expected_speed="å¿«",
            ),
            ModelConfig(
                name="ä¸­ç­‰æ€§èƒ½",
                det_arch="linknet_resnet18",
                reco_arch="crnn_vgg16_bn",
                description="ä¸­ç­‰æ€§èƒ½ï¼Œå¹³è¡¡è´¨é‡ä¸é€Ÿåº¦",
                expected_speed="ä¸­ç­‰",
            ),
            ModelConfig(
                name="é«˜è´¨é‡",
                det_arch="db_resnet50",
                reco_arch="vitstr_small",
                description="é«˜è´¨é‡è¯†åˆ«ï¼Œé€‚åˆç²¾åº¦è¦æ±‚é«˜çš„åœºæ™¯",
                expected_speed="æ…¢",
            ),
            ModelConfig(
                name="æœ€é«˜è´¨é‡",
                det_arch="linknet_resnet50",
                reco_arch="parseq",
                description="æœ€é«˜è´¨é‡ï¼Œé€‚åˆå¯¹ç²¾åº¦è¦æ±‚æé«˜çš„åœºæ™¯",
                expected_speed="å¾ˆæ…¢",
            ),
        ]

        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(output_dir, exist_ok=True)

    def setup_logging(self):
        """è®¾ç½®æ—¥å¿—"""
        log_file = os.path.join(self.output_dir, f"model_comparison_{int(time.time())}.log")
        setup_logging_with_file(log_file, encoding="utf-8")
        logging.info(f"æ—¥å¿—æ–‡ä»¶: {log_file}")

    def get_memory_usage(self) -> float:
        """è·å–å½“å‰å†…å­˜ä½¿ç”¨é‡(MB)"""
        try:
            process = psutil.Process()
            memory_info = process.memory_info()
            return memory_info.rss / (1024 * 1024)  # è½¬æ¢ä¸ºMB
        except:
            return 0.0

    def discover_test_files(self, input_path: str) -> list[str]:
        """å‘ç°æµ‹è¯•æ–‡ä»¶"""
        supported_extensions = {
            ".pdf",
            ".docx",
            ".pptx",
            ".xlsx",
            ".doc",
            ".ppt",
            ".xls",
            ".jpg",
            ".jpeg",
            ".png",
        }
        files = []

        if os.path.isfile(input_path):
            if Path(input_path).suffix.lower() in supported_extensions:
                files.append(input_path)
        elif os.path.isdir(input_path):
            for filename in sorted(os.listdir(input_path)):
                filepath = os.path.join(input_path, filename)
                if (
                    os.path.isfile(filepath)
                    and Path(filepath).suffix.lower() in supported_extensions
                ):
                    files.append(filepath)

        logging.info(f"å‘ç° {len(files)} ä¸ªæµ‹è¯•æ–‡ä»¶")
        return files

    def process_file_with_model(
        self, file_path: str, model_config: ModelConfig, run_number: int, use_cpu: bool = False
    ) -> TestResult:
        """ä½¿ç”¨æŒ‡å®šæ¨¡å‹å¤„ç†å•ä¸ªæ–‡ä»¶"""

        result = TestResult(
            model_name=model_config.name,
            file_path=file_path,
            file_name=os.path.basename(file_path),
            file_size=os.path.getsize(file_path),
            file_extension=Path(file_path).suffix.lower(),
            run_number=run_number,
            success=False,
            processing_time=0.0,
            quality_score=0.0,
            text_length=0,
            pages_processed=0,
            memory_usage_mb=0.0,
            quality_details={},
        )

        start_time = time.time()
        memory_before = self.get_memory_usage()

        try:
            # åŠ è½½OCRæ¨¡å‹
            logging.info(
                f"Run {run_number}: åŠ è½½æ¨¡å‹ {model_config.name} ({model_config.det_arch} + {model_config.reco_arch})"
            )
            model = common.load_ocr_model(model_config.det_arch, model_config.reco_arch, use_cpu)

            # å¤„ç†æ–‡ä»¶
            if file_path.lower().endswith(".pdf"):
                # PDFæ–‡ä»¶å¤„ç†
                doc = DocumentFile.from_pdf(file_path)
                ocr_result = model(doc)
                result.pages_processed = len(doc)

                # æå–æ–‡æœ¬
                text_content = []
                for page_idx, page in enumerate(ocr_result.pages, 1):
                    page_text = page.render()
                    text_content.append(f"## Page {page_idx}\n\n{page_text}")

                final_text = "\n\n".join(text_content)

            else:
                # å›¾ç‰‡æ–‡ä»¶å¤„ç†
                doc = DocumentFile.from_images([file_path])
                ocr_result = model(doc)
                result.pages_processed = 1

                # æå–æ–‡æœ¬
                page_text = ocr_result.pages[0].render()
                final_text = page_text

            # ä¿å­˜è¾“å‡ºæ–‡ä»¶ï¼ˆåªä¿å­˜ç¬¬äºŒæ¬¡è¿è¡Œçš„ç»“æœç”¨äºæ¯”è¾ƒï¼‰
            if run_number == 2:
                base_name = os.path.splitext(os.path.basename(file_path))[0]
                safe_model_name = model_config.name.replace("/", "_")
                output_filename = f"{base_name}_{safe_model_name}.md"
                output_path = os.path.join(self.output_dir, "markdown_outputs", output_filename)

                os.makedirs(os.path.dirname(output_path), exist_ok=True)
                with open(output_path, "w", encoding="utf-8") as f:
                    f.write(final_text)

            # è®¡ç®—è´¨é‡è¯„åˆ†
            quality_metrics = self.evaluator.calculate_text_quality_score(final_text)
            result.quality_score = quality_metrics["total_score"]
            result.quality_details = quality_metrics
            result.text_length = len(final_text)
            result.success = True

            # å†…å­˜ä½¿ç”¨
            memory_after = self.get_memory_usage()
            result.memory_usage_mb = max(0, memory_after - memory_before)

        except Exception as e:
            result.error_message = str(e)
            logging.error(f"Run {run_number}: å¤„ç†å¤±è´¥ {file_path} with {model_config.name}: {e}")

        finally:
            # æ¸…ç†å†…å­˜
            try:
                del model
                gc.collect()
            except:
                pass

        result.processing_time = time.time() - start_time
        return result

    def run_comparison_test(self, input_path: str, use_cpu: bool = False) -> dict[str, Any]:
        """è¿è¡Œå¯¹æ¯”æµ‹è¯•"""

        # å‘ç°æµ‹è¯•æ–‡ä»¶
        test_files = self.discover_test_files(input_path)
        if not test_files:
            logging.error("æœªæ‰¾åˆ°æµ‹è¯•æ–‡ä»¶")
            return {}

        logging.info(
            f"å¼€å§‹æ¨¡å‹å¯¹æ¯”æµ‹è¯•ï¼Œå…± {len(test_files)} ä¸ªæ–‡ä»¶ï¼Œ{len(self.model_configs)} ä¸ªæ¨¡å‹"
        )

        total_tests = len(test_files) * len(self.model_configs) * 2  # æ¯ä¸ªæ¨¡å‹è¿è¡Œ2æ¬¡
        current_test = 0

        # å¯¹æ¯ä¸ªæ–‡ä»¶å’Œæ¨¡å‹ç»„åˆè¿›è¡Œæµ‹è¯•
        for file_path in test_files:
            logging.info(f"\n=== æµ‹è¯•æ–‡ä»¶: {os.path.basename(file_path)} ===")

            for model_config in self.model_configs:
                logging.info(f"\n--- æ¨¡å‹: {model_config.name} ---")

                # è¿è¡Œä¸¤æ¬¡
                for run_number in [1, 2]:
                    current_test += 1
                    logging.info(f"è¿›åº¦: {current_test}/{total_tests} - Run {run_number}/2")

                    result = self.process_file_with_model(
                        file_path, model_config, run_number, use_cpu
                    )

                    self.results.append(result)

                    if result.success:
                        logging.info(
                            f"âœ“ æˆåŠŸ - æ—¶é—´: {result.processing_time:.2f}s, è´¨é‡: {result.quality_score:.1f}, æ–‡æœ¬é•¿åº¦: {result.text_length}"
                        )
                    else:
                        logging.info(f"âœ— å¤±è´¥ - {result.error_message}")

        # ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š
        return self.generate_report()

    def generate_report(self) -> dict[str, Any]:
        """ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š"""

        logging.info("\n" + "=" * 80)
        logging.info("OCRæ¨¡å‹å¯¹æ¯”æµ‹è¯•æŠ¥å‘Š")
        logging.info("=" * 80)

        # æŒ‰æ¨¡å‹åˆ†ç»„ç»“æœï¼ˆåªä½¿ç”¨ç¬¬äºŒæ¬¡è¿è¡Œçš„ç»“æœï¼Œæ’é™¤æ¨¡å‹ä¸‹è½½æ—¶é—´ï¼‰
        model_results = {}

        for result in self.results:
            if result.run_number == 2:  # åªä½¿ç”¨ç¬¬äºŒæ¬¡è¿è¡Œç»“æœ
                model_name = result.model_name
                if model_name not in model_results:
                    model_results[model_name] = []
                model_results[model_name].append(result)

        # è®¡ç®—æ¯ä¸ªæ¨¡å‹çš„ç»Ÿè®¡ä¿¡æ¯
        summary_data = []

        for model_config in self.model_configs:
            model_name = model_config.name
            results = model_results.get(model_name, [])

            if not results:
                continue

            successful_results = [r for r in results if r.success]

            summary = {
                "model_name": model_name,
                "det_arch": model_config.det_arch,
                "reco_arch": model_config.reco_arch,
                "description": model_config.description,
                "expected_speed": model_config.expected_speed,
                "total_files": len(results),
                "successful_files": len(successful_results),
                "success_rate": len(successful_results) / len(results) * 100 if results else 0,
                "avg_processing_time": sum(r.processing_time for r in successful_results)
                / len(successful_results)
                if successful_results
                else 0,
                "avg_quality_score": sum(r.quality_score for r in successful_results)
                / len(successful_results)
                if successful_results
                else 0,
                "avg_text_length": sum(r.text_length for r in successful_results)
                / len(successful_results)
                if successful_results
                else 0,
                "avg_memory_usage": sum(r.memory_usage_mb for r in successful_results)
                / len(successful_results)
                if successful_results
                else 0,
                "total_pages_processed": sum(r.pages_processed for r in successful_results),
            }

            summary_data.append(summary)

        # æŒ‰å¹³å‡å¤„ç†æ—¶é—´æ’åº
        summary_data.sort(key=lambda x: x["avg_processing_time"])

        # æ‰“å°è¡¨æ ¼æŠ¥å‘Š
        self.print_table_report(summary_data)

        # ä¿å­˜è¯¦ç»†ç»“æœ
        report_data = {
            "timestamp": int(time.time()),
            "test_type": "ocr_model_comparison",
            "model_configs": [
                {
                    "name": config.name,
                    "det_arch": config.det_arch,
                    "reco_arch": config.reco_arch,
                    "description": config.description,
                    "expected_speed": config.expected_speed,
                }
                for config in self.model_configs
            ],
            "summary": summary_data,
            "detailed_results": [
                {
                    "model_name": r.model_name,
                    "file_name": r.file_name,
                    "file_size": r.file_size,
                    "file_extension": r.file_extension,
                    "run_number": r.run_number,
                    "success": r.success,
                    "processing_time": r.processing_time,
                    "quality_score": r.quality_score,
                    "text_length": r.text_length,
                    "pages_processed": r.pages_processed,
                    "memory_usage_mb": r.memory_usage_mb,
                    "error_message": r.error_message,
                    "quality_details": r.quality_details,
                }
                for r in self.results
            ],
        }

        # ä¿å­˜JSONæŠ¥å‘Š
        report_file = os.path.join(
            self.output_dir, f"model_comparison_report_{report_data['timestamp']}.json"
        )
        with open(report_file, "w", encoding="utf-8") as f:
            json.dump(report_data, f, indent=2, ensure_ascii=False)

        logging.info(f"\nè¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")

        return report_data

    def print_table_report(self, summary_data: list[dict]):
        """æ‰“å°è¡¨æ ¼æ ¼å¼çš„æŠ¥å‘Š"""

        print("\n" + "=" * 120)
        print("æ¨¡å‹æ€§èƒ½å¯¹æ¯”è¡¨æ ¼")
        print("=" * 120)

        # è¡¨å¤´
        headers = [
            "æ¨¡å‹åç§°",
            "æ£€æµ‹+è¯†åˆ«æ¶æ„",
            "æˆåŠŸç‡",
            "å¹³å‡æ—¶é—´(s)",
            "è´¨é‡è¯„åˆ†",
            "å¹³å‡æ–‡æœ¬é•¿åº¦",
            "å†…å­˜ä½¿ç”¨(MB)",
            "é€‚ç”¨åœºæ™¯",
        ]

        # è®¡ç®—åˆ—å®½
        col_widths = [12, 35, 8, 12, 10, 12, 12, 20]

        # æ‰“å°è¡¨å¤´
        header_row = "| "
        for i, header in enumerate(headers):
            header_row += f"{header:^{col_widths[i]}} | "
        print(header_row)

        # åˆ†éš”çº¿
        separator = "|"
        for width in col_widths:
            separator += "-" * (width + 2) + "|"
        print(separator)

        # æ•°æ®è¡Œ
        for data in summary_data:
            model_name = data["model_name"][:10]
            arch_combo = f"{data['det_arch']}+{data['reco_arch']}"[:33]
            success_rate = f"{data['success_rate']:.1f}%"
            avg_time = f"{data['avg_processing_time']:.2f}"
            quality = f"{data['avg_quality_score']:.1f}"
            text_len = (
                f"{int(data['avg_text_length'] / 1000)}k" if data["avg_text_length"] > 0 else "0"
            )
            memory = f"{data['avg_memory_usage']:.1f}"
            scenario = self._get_scenario_recommendation(data)[:18]

            row = (
                f"| {model_name:^{col_widths[0]}} | {arch_combo:<{col_widths[1]}} | "
                + f"{success_rate:^{col_widths[2]}} | {avg_time:^{col_widths[3]}} | "
                + f"{quality:^{col_widths[4]}} | {text_len:^{col_widths[5]}} | "
                + f"{memory:^{col_widths[6]}} | {scenario:<{col_widths[7]}} |"
            )
            print(row)

        print("=" * 120)

        # æ·»åŠ å»ºè®®è¯´æ˜
        print("\nğŸ“‹ ä½¿ç”¨å»ºè®®:")
        print("â€¢ ç§»åŠ¨ç«¯/è¾¹ç¼˜è®¾å¤‡: é€‰æ‹©å¤„ç†æ—¶é—´æœ€çŸ­çš„æ¨¡å‹")
        print("â€¢ æ—¥å¸¸æ‰¹é‡å¤„ç†: é€‰æ‹©æ—¶é—´å’Œè´¨é‡å¹³è¡¡çš„æ¨¡å‹")
        print("â€¢ é«˜ç²¾åº¦éœ€æ±‚: é€‰æ‹©è´¨é‡è¯„åˆ†æœ€é«˜çš„æ¨¡å‹")
        print("â€¢ å†…å­˜å—é™ç¯å¢ƒ: é€‰æ‹©å†…å­˜ä½¿ç”¨æœ€å°‘çš„æ¨¡å‹")

    def _get_scenario_recommendation(self, data: dict) -> str:
        """æ ¹æ®æ¨¡å‹æ•°æ®æ¨èä½¿ç”¨åœºæ™¯"""
        if data["avg_processing_time"] < 5:
            return "ç§»åŠ¨ç«¯/å¿«é€Ÿå¤„ç†"
        elif data["avg_processing_time"] < 15:
            return "æ—¥å¸¸ä½¿ç”¨"
        elif data["avg_quality_score"] > 60:
            return "é«˜ç²¾åº¦éœ€æ±‚"
        else:
            return "ç‰¹æ®Šç”¨é€”"


def create_parser() -> argparse.ArgumentParser:
    """åˆ›å»ºå‚æ•°è§£æå™¨"""
    parser = argparse.ArgumentParser(
        description="OCRæ¨¡å‹è´¨é‡å¯¹æ¯”æµ‹è¯•", formatter_class=argparse.RawDescriptionHelpFormatter
    )

    parser.add_argument("input_path", help="æµ‹è¯•æ–‡ä»¶è·¯å¾„æˆ–åŒ…å«æµ‹è¯•æ–‡ä»¶çš„ç›®å½•")

    parser.add_argument(
        "--output-dir",
        default="model_comparison_results",
        help="ç»“æœè¾“å‡ºç›®å½• (é»˜è®¤: model_comparison_results)",
    )

    parser.add_argument("--cpu", action="store_true", help="å¼ºåˆ¶ä½¿ç”¨CPUè¿›è¡ŒOCRå¤„ç†")

    return parser


def main():
    """ä¸»å‡½æ•°"""
    parser = create_parser()
    args = parser.parse_args()

    # åˆ›å»ºæµ‹è¯•å™¨
    tester = ModelComparisonTester(args.output_dir)
    tester.setup_logging()

    logging.info("å¼€å§‹OCRæ¨¡å‹è´¨é‡å¯¹æ¯”æµ‹è¯•")
    logging.info(f"è¾“å…¥è·¯å¾„: {args.input_path}")
    logging.info(f"è¾“å‡ºç›®å½•: {args.output_dir}")
    logging.info(f"ä½¿ç”¨CPU: {'æ˜¯' if args.cpu else 'å¦'}")

    try:
        # è¿è¡Œå¯¹æ¯”æµ‹è¯•
        results = tester.run_comparison_test(args.input_path, args.cpu)

        if results:
            logging.info("\nğŸ‰ OCRæ¨¡å‹å¯¹æ¯”æµ‹è¯•å®Œæˆ!")
        else:
            logging.error("æµ‹è¯•å¤±è´¥æˆ–æ— ç»“æœ")

    except KeyboardInterrupt:
        logging.info("ç”¨æˆ·ä¸­æ–­æµ‹è¯•")
        sys.exit(1)
    except Exception as e:
        logging.error(f"æµ‹è¯•å¤±è´¥: {e}")
        logging.debug(traceback.format_exc())
        sys.exit(1)


if __name__ == "__main__":
    main()
